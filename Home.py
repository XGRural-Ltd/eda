import streamlit as st
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import kagglehub

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score

@st.cache_data
def load_data():
    path = kagglehub.dataset_download("maharshipandya/-spotify-tracks-dataset")
    path_dataset = path + '/dataset.csv'
    df = pd.read_csv(path_dataset, index_col=0)
    if 'track_genre' not in df.columns:
        df['track_genre'] = 'unknown'
    return df

df = load_data()

num_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()


def pagina_1_visao_geral(df):
    st.subheader("üìä Informa√ß√µes Gerais do Dataset")
    st.write("Nesta etapa vamos ficar mais familiarizados com os dados. Vamos explorar as colunas, tipos de dados, valores ausentes, estat√≠sticas descritivas e visualizar alguns plots.")

    num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
    

    st.markdown("**Visualize o DataFrame com as colunas selecionadas:**")
    cols = st.multiselect("Colunas para exibir:", df.columns.tolist(), default=df.columns[:6].tolist())
    st.dataframe(df[cols].head(15))

    
    st.markdown("**Tipos de dados e valores ausentes:**")
    if st.checkbox("Mostrar dtypes e valores nulos"):
        col1, col2 = st.columns(2)
        with col1:
            st.write(df.dtypes.astype(str))
        with col2:
            st.write(df.isnull().sum())

    st.markdown("**Estat√≠sticas descritivas:**")
    if st.checkbox("Exibir estat√≠sticas descritivas"):
        st.write(df.describe())

    st.markdown("---")
    st.subheader("üßæ Dicion√°rio de Dados: Descri√ß√£o das Colunas")

    col_descriptions = {
        "track_id": "O ID do Spotify para a faixa.",
        "artists": "Nomes dos artistas que performaram a faixa. Se houver mais de um, s√£o separados por ponto e v√≠rgula.",
        "album_name": "Nome do √°lbum no qual a faixa aparece.",
        "track_name": "Nome da faixa.",
        "popularity": "Popularidade da faixa (0 a 100), baseada em n√∫mero e rec√™ncia de reprodu√ß√µes.",
        "duration_ms": "Dura√ß√£o da faixa em milissegundos.",
        "explicit": "Indica se a faixa possui conte√∫do expl√≠cito (True = sim, False = n√£o).",
        "danceability": "Qu√£o dan√ßante √© a faixa, de 0.0 (menos) a 1.0 (mais dan√ßante).",
        "energy": "Energia percebida da faixa, de 0.0 a 1.0.",
        "key": "Tom da m√∫sica (0 = D√≥, 1 = D√≥‚ôØ/R√©‚ô≠, ..., -1 = indetect√°vel).",
        "loudness": "Volume geral da faixa em decib√©is (dB).",
        "mode": "Modalidade: 1 = maior, 0 = menor.",
        "speechiness": "Detecta presen√ßa de fala. 1.0 = fala pura; 0.0 = m√∫sica pura.",
        "acousticness": "Confian√ßa de que a faixa √© ac√∫stica (0.0 a 1.0).",
        "instrumentalness": "Probabilidade de n√£o conter vocais. Pr√≥ximo de 1.0 = instrumental.",
        "liveness": "Probabilidade de ter sido gravada ao vivo. Acima de 0.8 = performance ao vivo.",
        "valence": "Qu√£o positiva √© a m√∫sica (0.0 = triste, 1.0 = alegre).",
        "tempo": "Tempo estimado da faixa (batidas por minuto).",
        "time_signature": "Compasso estimado (de 3 a 7).",
        "track_genre": "G√™nero musical da faixa."
    }

    available_columns = list(col_descriptions.keys())
    selected_columns = st.multiselect("Selecione as colunas que deseja entender melhor:", available_columns)

    if selected_columns:
        for col in selected_columns:
            st.markdown(f"**üîπ {col}**: {col_descriptions[col]}")
    
    st.markdown("---")
    st.subheader("üìà Visualiza√ß√µes Gerais de Distribui√ß√£o")

    selected_col = st.selectbox("Selecione uma coluna num√©rica:", num_cols)
    plot_type = st.radio("Tipo de gr√°fico:", ["Histograma", "Boxplot", "Ambos"])

    fig, axs = plt.subplots(1, 2 if plot_type == "Ambos" else 1, figsize=(14, 4))
    if plot_type == "Histograma" or plot_type == "Ambos":
        ax = axs[0] if plot_type == "Ambos" else axs
        sns.histplot(df[selected_col], kde=True, ax=ax, color='skyblue')
        ax.set_title(f"Histograma de {selected_col}")
    if plot_type == "Boxplot" or plot_type == "Ambos":
        ax = axs[1] if plot_type == "Ambos" else axs
        sns.boxplot(x=df[selected_col], ax=ax, color='salmon')
        ax.set_title(f"Boxplot de {selected_col}")
    st.pyplot(fig)

    st.markdown("---")
    st.subheader("üìà Gr√°ficos de Dispers√£o entre Vari√°veis Num√©ricas")
    x_axis = st.selectbox("Vari√°vel no eixo X:", num_features, index=0)
    y_axis = st.selectbox("Vari√°vel no eixo Y:", num_features, index=1)
    show_trend = st.checkbox("Mostrar linha de tend√™ncia (regress√£o linear)")
    

    df_plot = df[[x_axis, y_axis]].dropna()

    fig, ax = plt.subplots()
    if show_trend:
        sns.regplot(data=df_plot, x=x_axis, y=y_axis, ax=ax,
                    scatter_kws={'alpha': 0.5, 'color': 'red'},
                    line_kws={"color": "blue"})
    else:
        sns.scatterplot(data=df_plot, x=x_axis, y=y_axis, alpha=0.5, color='red', ax=ax)

    ax.set_title(f"Dispers√£o entre {x_axis} e {y_axis}")
    st.pyplot(fig)
    st.markdown("üìå Danceability vs. Energy")
    st.markdown("- J√° esperamnos uma correla√ß√£o positiva entre 'danceability' e 'energy', pois m√∫sicas mais dan√ßantes tendem a ter mais energia.  \n"
                "- A linha de tend√™ncia (regress√£o linear) ajuda a visualizar uma correla√ß√£o moderadamente positiva. \n"
                "\n" 
                "üìå Acousticness vs. Energy \n"
                "- Correla√ß√£o negativa forte esperada, pois m√∫sicas ac√∫sticas s√£o menos energ√©ticas \n"
                "- A linha de tend√™ncia decrescemte indica uma rela√ß√£o inversamente proporcional. \n"
                "\n"
                "üìå Loudness vs. Energy \n"
                "- Baixa dispers√£o e ascend√™ncia dos pontos mostram uma correla√ß√£o fortemente positiva (m√∫sicas energ√©ticas costumam ser mais altas) \n")

def pagina_2_analise_univariada(df):
    st.subheader("üî¨ An√°lise Univariada Detalhada")
    st.markdown("Explore a distribui√ß√£o de cada vari√°vel. Use os filtros para comparar diferentes g√™neros e ajuste os gr√°ficos para uma an√°lise mais profunda.")

    st.markdown("### üé≠ Comparar Distribui√ß√µes por G√™nero")
    genres_to_compare = st.multiselect(
        "Selecione um ou mais g√™neros para comparar (opcional):",
        sorted(df['track_genre'].unique().tolist())
    )

    if genres_to_compare:
        df_filtered = df[df['track_genre'].isin(genres_to_compare)]
        hue_on = 'track_genre'
    else:
        df_filtered = df.copy()
        hue_on = None

    st.markdown("### ‚öôÔ∏è Controles da An√°lise")
    selected_var = st.selectbox("Selecione uma vari√°vel num√©rica para an√°lise:", num_features)

    num_bins = st.slider("N√∫mero de Bins para o Histograma:", min_value=10, max_value=100, value=30)
    
    st.markdown(f"---\n### üéØ An√°lise da vari√°vel: `{selected_var}`")

    st.markdown("**üìä Visualiza√ß√£o da Distribui√ß√£o:**")
    col1, col2 = st.columns(2)
    with col1:
        fig1, ax1 = plt.subplots()
        sns.histplot(data=df_filtered, x=selected_var, hue=hue_on, kde=True, ax=ax1, bins=num_bins, palette='viridis')
        ax1.set_title(f"Distribui√ß√£o de {selected_var}")
        st.pyplot(fig1)

    with col2:
        fig2, ax2 = plt.subplots()
        sns.boxplot(data=df_filtered, x=selected_var, y=hue_on, ax=ax2, palette='viridis', orient='h')
        ax2.set_title(f"Boxplot de {selected_var}")
        st.pyplot(fig2)

    st.markdown(f"**üîé Estat√≠sticas e Outliers para `{selected_var}`**")
    if genres_to_compare:
        st.write("Estat√≠sticas descritivas por g√™nero selecionado:")
        st.write(df_filtered.groupby('track_genre')[selected_var].describe().T)
    else:
        st.write("Estat√≠sticas descritivas gerais:")
        st.write(df_filtered[selected_var].describe())
    
    if st.checkbox(f"Mostrar outliers (m√∫sicas com valores extremos) para '{selected_var}'"):
        q1 = df_filtered[selected_var].quantile(0.25)
        q3 = df_filtered[selected_var].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        outliers = df_filtered[(df_filtered[selected_var] < lower_bound) | (df_filtered[selected_var] > upper_bound)]
        
        if outliers.empty:
            st.success("N√£o foram encontrados outliers com base no crit√©rio de 1.5 * IQR. ‚ú®")
        else:
            st.write(f"Foram encontrados **{len(outliers)}** outliers:")
            st.dataframe(outliers[['track_name', 'artists', 'track_genre', selected_var]].sort_values(by=selected_var, ascending=False))

def pagina_3_correlacao(df):
    st.subheader("‚ÜîÔ∏è An√°lise de Correla√ß√£o")
    st.markdown("Investigue a rela√ß√£o entre as vari√°veis. Use o filtro de g√™nero e o slider de intensidade para focar nas correla√ß√µes mais importantes.")
    
    st.markdown("### üéµ Filtrar por G√™nero")
    genre_list = ['Todos'] + sorted(df['track_genre'].unique().tolist())
    corr_genre = st.selectbox("Selecione um g√™nero para calcular a correla√ß√£o:", genre_list)

    if corr_genre == 'Todos':
        df_corr = df[num_features]
    else:
        df_corr = df[df['track_genre'] == corr_genre][num_features]
        st.info(f"Mostrando correla√ß√µes apenas para o g√™nero: **{corr_genre}**")

    st.markdown("### ‚öôÔ∏è Controles do Heatmap")
    corr_method = st.selectbox("M√©todo de correla√ß√£o:", ["pearson", "spearman", "kendall"])
    
    corr_threshold = st.slider("Ocultar no gr√°fico e na tabela correla√ß√µes com valor absoluto abaixo de:", 0.0, 1.0, 0.0, 0.05)
    
    if not df_corr.empty and len(df_corr) > 1:
        corr_matrix = df_corr.corr(method=corr_method)
        
        mask_upper = np.triu(np.ones_like(corr_matrix, dtype=bool))
        mask_threshold = np.abs(corr_matrix) < corr_threshold
        mask_heatmap = mask_upper | mask_threshold
        
        fig, ax = plt.subplots(figsize=(12, 9))
        sns.heatmap(corr_matrix, mask=mask_heatmap, annot=True, cmap='coolwarm', fmt=".2f", ax=ax, annot_kws={"size": 8}, vmin=-1, vmax=1)
        ax.set_title(f"Mapa de Calor (M√©todo: {corr_method.capitalize()}, G√™nero: {corr_genre})", fontsize=16)
        st.pyplot(fig)

        st.markdown("### ‚ú® Pares com Maior Correla√ß√£o")
        st.write(f"Abaixo est√£o os pares de vari√°veis com correla√ß√£o absoluta acima de **{corr_threshold}** (excluindo duplicatas e auto-correla√ß√µes).")
        
        mask_table = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
        corr_unstacked = corr_matrix.where(mask_table).stack()
        
        strong_pairs = corr_unstacked.sort_values(key=abs, ascending=False)
        
        strong_pairs = strong_pairs[abs(strong_pairs) > corr_threshold]

        if strong_pairs.empty:
            st.warning("Nenhum par encontrado acima do threshold. Tente um valor menor.")
        else:
            st.dataframe(strong_pairs.to_frame(name='correlation_value').head(20))

    else:
        st.warning(f"N√£o h√° dados suficientes para o g√™nero '{corr_genre}' para calcular a correla√ß√£o.")

def pagina_4_outliers(df):
    st.subheader("üö® Detec√ß√£o de Outliers com Isolation Forest")
    st.markdown("""
    Nesta se√ß√£o, vamos identificar **outliers**: m√∫sicas que possuem caracter√≠sticas muito diferentes da maioria. Um outlier pode ser uma m√∫sica experimental, um erro nos dados ou simplesmente uma faixa √∫nica.
    
    Usaremos o algoritmo **Isolation Forest**, que √© eficiente em detectar anomalias em dados multidimensionais. Ele funciona isolando observa√ß√µes ao selecionar aleatoriamente uma feature e, em seguida, um valor de divis√£o aleat√≥rio entre os valores m√°ximo e m√≠nimo da feature selecionada.
    """)

    st.markdown("### ‚öôÔ∏è Controles da Detec√ß√£o")
    features_for_outliers = st.multiselect(
        "Selecione as features para a detec√ß√£o de outliers:", 
        num_features, 
        default=['danceability', 'energy', 'loudness', 'acousticness', 'valence']
    )
    
    if not features_for_outliers:
        st.warning("Por favor, selecione ao menos uma feature para a an√°lise.")
        return

    contamination = st.slider(
        "Taxa de contamina√ß√£o (propor√ß√£o de outliers):", 
        0.01, 0.2, 0.05, step=0.01,
        help="Este valor representa a propor√ß√£o esperada de outliers no dataset. Um valor maior resultar√° em mais m√∫sicas sendo classificadas como anomalias."
    )

    st.markdown("---")
    st.markdown("### üéº M√∫sicas An√¥malas Detectadas")

    df_outlier_analysis = df.copy()

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_outlier_analysis[features_for_outliers])

    iso = IsolationForest(contamination=contamination, random_state=42)
    df_outlier_analysis['anomaly'] = iso.fit_predict(X_scaled)
    outliers = df_outlier_analysis[df_outlier_analysis['anomaly'] == -1]

    st.write(f"Com base nas suas configura√ß√µes, foram detectadas **{len(outliers)}** m√∫sicas como outliers.")
    st.dataframe(outliers[['track_name', 'artists', 'track_genre'] + features_for_outliers])

    if len(features_for_outliers) >= 2:
        st.subheader("üìç Visualiza√ß√£o de Outliers em 2D (usando PCA)")
        st.markdown("""
        Para visualizar os outliers em um gr√°fico 2D, reduzimos a dimensionalidade das features selecionadas usando **An√°lise de Componentes Principais (PCA)**. Os pontos em vermelho representam as m√∫sicas marcadas como outliers.
        """)
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_scaled)
        
        df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=df_outlier_analysis.index)
        df_pca['anomaly'] = df_outlier_analysis['anomaly']
        
        fig, ax = plt.subplots(figsize=(10, 7))
        sns.scatterplot(
            data=df_pca, x='PC1', y='PC2', hue='anomaly', 
            palette={1: 'blue', -1: 'red'}, style='anomaly',
            markers={1: '.', -1: 'X'}, s=100, alpha=0.7, ax=ax
        )
        ax.set_title("Outliers detectados via PCA")
        ax.legend(['Normal', 'Outlier'])
        st.pyplot(fig)
    else:
        st.info("Selecione 2 ou mais features para visualizar o gr√°fico de dispers√£o com PCA.")

@st.cache_data
def convert_df_to_csv(df):
    return df.to_csv(index=False).encode('utf-8')

def pagina_5_preprocessamento(df):
    st.subheader("‚öôÔ∏è 5. Pr√©-processamento dos Dados")
    st.markdown("""
    O pr√©-processamento √© uma etapa fundamental na prepara√ß√£o de dados para modelos de Machine Learning. Aqui, transformaremos nossas features para que os algoritmos possam interpret√°-las da melhor forma poss√≠vel.
    
    Vamos abordar tr√™s etapas principais:
    1.  **Imputa√ß√£o de Dados**: Substituir valores ausentes.
    2.  **Feature Scaling**: Padronizar as escalas das nossas vari√°veis num√©ricas.
    3.  **One-Hot Encoding**: Converter vari√°veis categ√≥ricas em um formato num√©rico.
    """)

    df_processed = df.copy()

    st.markdown("---")
    st.markdown("### 1. Tratamento de Valores Ausentes")
    st.markdown("""
    Antes de escalar os dados, precisamos lidar com valores ausentes (NaN). Uma estrat√©gia comum e robusta √© a **imputa√ß√£o pela mediana**, onde substitu√≠mos os valores ausentes pelo valor central da coluna. Isso √© menos sens√≠vel a outliers do que usar a m√©dia.
    """)
    
    # Preencher NaNs com a mediana das colunas num√©ricas
    numeric_cols_with_na = df_processed[num_features].columns[df_processed[num_features].isnull().any()].tolist()
    if numeric_cols_with_na:
        st.write("Valores ausentes encontrados nas seguintes colunas e preenchidos com a mediana:")
        st.write(df_processed[numeric_cols_with_na].isnull().sum().to_frame(name='NAs Preenchidos'))
        df_processed.fillna(df_processed.median(numeric_only=True), inplace=True)
    else:
        st.success("Nenhum valor ausente encontrado nas colunas num√©ricas. ‚úÖ")
    

    st.markdown("---")
    st.markdown("### 2. Feature Scaling (Padroniza√ß√£o)")
    st.markdown("""
    Algoritmos de clusteriza√ß√£o, como o K-Means, s√£o sens√≠veis √† escala das features. Vari√°veis com escalas maiores (como `duration_ms`) podem dominar o processo de agrupamento.
    Usaremos o **StandardScaler**, que transforma os dados para que tenham m√©dia 0 e desvio padr√£o 1.
    """)

    features_to_scale = st.multiselect(
        "Selecione as features num√©ricas para padronizar:",
        options=num_features,
        default=num_features
    )
    
    if features_to_scale:
        scaler = StandardScaler()
        df_processed[features_to_scale] = scaler.fit_transform(df_processed[features_to_scale])

        st.markdown("**Compara√ß√£o: Antes vs. Depois da Padroniza√ß√£o** (para a feature `danceability`)")
        col1, col2 = st.columns(2)
        with col1:
            st.write("Antes:")
            fig, ax = plt.subplots(figsize=(6,4))
            sns.histplot(df['danceability'], kde=True, ax=ax, color='blue')
            ax.set_title("Original")
            st.pyplot(fig)
        with col2:
            st.write("Depois:")
            fig, ax = plt.subplots(figsize=(6,4))
            sns.histplot(df_processed['danceability'], kde=True, ax=ax, color='green')
            ax.set_title("Padronizado")
            st.pyplot(fig)
    
    st.markdown("---")
    st.markdown("### 3. One-Hot Encoding para G√™neros")
    st.markdown("""
    Para usar a feature `track_genre` em nosso modelo, precisamos convert√™-la de texto para um formato num√©rico. O **One-Hot Encoding** cria novas colunas para cada g√™nero, marcando com `1` se a m√∫sica pertence √†quele g√™nero e `0` caso contr√°rio.
    """)
    
    if st.checkbox("Aplicar One-Hot Encoding na coluna 'track_genre'?", value=True):
        df_processed = pd.get_dummies(df_processed, columns=['track_genre'], prefix='genre')
        st.success(f"One-Hot Encoding aplicado! Novas colunas de g√™nero foram criadas.")
    
    st.markdown("---")
    st.markdown("### üèÅ DataFrame Final Pr√©-processado")
    st.markdown("Abaixo est√° uma amostra do nosso dataset ap√≥s as transforma√ß√µes. Este √© o conjunto de dados que usaremos para a clusteriza√ß√£o.")
    
    final_features = df_processed.select_dtypes(include=np.number).columns.tolist()
    final_df = df_processed[final_features]

    st.dataframe(final_df.head())
    st.write(f"O dataset final possui **{final_df.shape[0]}** linhas e **{final_df.shape[1]}** features.")

    st.markdown("### üíæ Baixar Dados Processados")
    st.markdown("Clique no bot√£o para baixar o DataFrame processado em um arquivo CSV para uso posterior.")
    
    csv = convert_df_to_csv(final_df)
    st.download_button(
       label="Baixar dados como CSV",
       data=csv,
       file_name='processed_spotify_data.csv',
       mime='text/csv',
    )
    if st.button("Salvar DataFrame em cache para pr√≥ximas etapas"):
        st.session_state['processed_df'] = final_df
        st.success("DataFrame processado salvo na sess√£o! ‚úÖ")

def pagina_6_reducao_dimensionalidade(df):
    st.subheader("üìâ 6. Redu√ß√£o de Dimensionalidade")
    st.markdown("""
    Com um grande n√∫mero de features, pode ser dif√≠cil visualizar e modelar os dados. A **Redu√ß√£o de Dimensionalidade** nos ajuda a "comprimir" as informa√ß√µes mais importantes em um n√∫mero menor de componentes.
    
    Usaremos a **An√°lise de Componentes Principais (PCA)**, uma t√©cnica popular que encontra novas eixos (componentes) que maximizam a vari√¢ncia nos dados.
    """)
    
    if 'processed_df' not in st.session_state or st.session_state['processed_df'] is None:
        st.warning("Por favor, execute o pr√©-processamento na p√°gina '5. Pr√©-processamento' e clique em 'Salvar DataFrame' antes de continuar.")
        return

    processed_df = st.session_state['processed_df']
    
    st.markdown("### ‚öôÔ∏è Configurando o PCA")
    n_components = st.slider(
        "N√∫mero de componentes principais para gerar:",
        min_value=2, max_value=20, value=10,
        help="Escolha quantos componentes (novas features) voc√™ deseja criar. Come√ßar com 10 a 15 √© geralmente um bom ponto de partida."
    )
    
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(processed_df)

    st.markdown("### üìä Vari√¢ncia Explicada")
    st.markdown("O gr√°fico abaixo mostra quanta da vari√¢ncia original dos dados √© 'capturada' por cada componente principal. O ideal √© que os primeiros componentes capturem a maior parte da informa√ß√£o.")

    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(range(1, n_components + 1), explained_variance, alpha=0.6, color='b', label='Vari√¢ncia Individual')
    ax.plot(range(1, n_components + 1), cumulative_variance, 'r-o', label='Vari√¢ncia Cumulativa')
    ax.set_xlabel('Componentes Principais')
    ax.set_ylabel('Propor√ß√£o da Vari√¢ncia Explicada')
    ax.set_title('Vari√¢ncia Explicada pelos Componentes Principais')
    ax.legend(loc='best')
    ax.set_xticks(range(1, n_components + 1))
    st.pyplot(fig)

    st.info(f"Com **{n_components}** componentes, conseguimos explicar **{cumulative_variance[-1]:.2%}** da vari√¢ncia total dos dados.")
    
    st.markdown("### üíø Dados Transformados pelo PCA")
    st.markdown("Abaixo est√° o nosso dataset transformado, agora com um n√∫mero reduzido de dimens√µes. Estes ser√£o os dados que usaremos para a clusteriza√ß√£o.")
    
    df_pca = pd.DataFrame(X_pca, columns=[f'PC_{i+1}' for i in range(n_components)])
    st.dataframe(df_pca.head())

    if st.button("Salvar dados do PCA para pr√≥ximas etapas"):
        st.session_state['pca_df'] = df_pca
        st.success("Dados transformados pelo PCA salvos na sess√£o! ‚úÖ")


def pagina_7_clusterizacao(df):
    st.subheader("üß© 7. Clusteriza√ß√£o")
    st.markdown("""
    Agora que nossos dados est√£o preparados, vamos aplicar algoritmos de **clusteriza√ß√£o** para encontrar grupos (clusters) de m√∫sicas com caracter√≠sticas semelhantes. O objetivo √© descobrir "playlists" naturais escondidas nos dados.
    """)

    if 'pca_df' not in st.session_state or st.session_state['pca_df'] is None:
        st.warning("Por favor, execute a Redu√ß√£o de Dimensionalidade na p√°gina '6. Redu√ß√£o de Dimensionalidade' e salve os dados antes de continuar.")
        return
    
    X_data = st.session_state['pca_df']

    st.markdown("### ü§ñ Escolha do Algoritmo de Clusteriza√ß√£o")
    algo_choice = st.selectbox(
        "Selecione o algoritmo:",
        ["K-Means", "DBSCAN", "Clustering Aglomerativo"]
    )

    model = None
    labels = None

    if algo_choice == "K-Means":
        st.markdown("""
        **K-Means** √© um dos algoritmos mais populares. Ele agrupa os dados tentando separar as amostras em *k* grupos de vari√¢ncia igual, minimizando um crit√©rio conhecido como in√©rcia. Voc√™ precisa definir o n√∫mero de clusters (k) antecipadamente.
        """)
        k = st.slider("N√∫mero de clusters (k):", min_value=2, max_value=20, value=8)
        model = KMeans(n_clusters=k, random_state=42, n_init=10)
    
    elif algo_choice == "DBSCAN":
        st.markdown("""
        **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) agrupa pontos que est√£o densamente compactados, marcando como outliers os pontos que est√£o sozinhos em regi√µes de baixa densidade. √â √≥timo para encontrar clusters de formas arbitr√°rias e n√£o exige que voc√™ defina o n√∫mero de clusters.
        """)
        eps = st.slider("Epsilon (eps - raio da vizinhan√ßa):", min_value=0.1, max_value=5.0, value=1.5, step=0.1)
        min_samples = st.slider("N√∫mero M√≠nimo de Amostras (min_samples):", min_value=1, max_value=50, value=10)
        model = DBSCAN(eps=eps, min_samples=min_samples)

    elif algo_choice == "Clustering Aglomerativo":
        st.markdown("""
        O **Clustering Aglomerativo** realiza uma clusteriza√ß√£o hier√°rquica. Ele come√ßa tratando cada ponto como um cluster separado e, em seguida, mescla recursivamente os pares de clusters mais pr√≥ximos at√© que um certo n√∫mero de clusters seja alcan√ßado.
        """)
        n_clusters_agg = st.slider("N√∫mero de clusters:", min_value=2, max_value=20, value=8)
        model = AgglomerativeClustering(n_clusters=n_clusters_agg)

    if st.button(f"Executar {algo_choice}"):
        with st.spinner("Clusterizando os dados... Isso pode levar um momento."):
            labels = model.fit_predict(X_data)
            st.session_state['cluster_labels'] = labels
            st.session_state['cluster_data'] = X_data
            st.success(f"Clusteriza√ß√£o com {algo_choice} conclu√≠da! Os resultados foram salvos.")
            
            n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)
            st.write(f"N√∫mero de clusters encontrados: **{n_clusters_found}**")
            if -1 in labels:
                noise_points = np.sum(labels == -1)
                st.write(f"N√∫mero de pontos de ru√≠do (outliers): **{noise_points}**")

def pagina_8_avaliacao_clusters(df):
    st.subheader("üèÜ 8. Avalia√ß√£o dos Clusters")
    st.markdown("""
    Como saber se os clusters que encontramos s√£o bons? Nesta etapa, vamos usar m√©tricas quantitativas e visualiza√ß√µes para avaliar a qualidade dos nossos agrupamentos.
    """)

    if 'cluster_labels' not in st.session_state or st.session_state['cluster_labels'] is None or \
       'cluster_data' not in st.session_state or st.session_state['cluster_data'] is None:
        st.warning("Por favor, execute a Clusteriza√ß√£o na p√°gina '7. Clusteriza√ß√£o' antes de continuar.")
        return
        
    labels = st.session_state['cluster_labels']
    data = st.session_state['cluster_data']
    
    # Filtrar pontos de ru√≠do (comuns no DBSCAN) para as m√©tricas
    if -1 in labels:
        mask = labels != -1
        filtered_data = data[mask]
        filtered_labels = labels[mask]
    else:
        filtered_data = data
        filtered_labels = labels

    if len(set(filtered_labels)) < 2:
        st.error("A avalia√ß√£o requer pelo menos 2 clusters (excluindo ru√≠do). O algoritmo pode n√£o ter encontrado grupos significativos com os par√¢metros atuais.")
        return

    st.markdown("### üìà M√©tricas de Avalia√ß√£o")
    col1, col2 = st.columns(2)
    with col1:
        st.metric(
            label="Silhouette Score",
            value=f"{silhouette_score(filtered_data, filtered_labels):.3f}",
            help="Mede o qu√£o semelhante um objeto √© ao seu pr√≥prio cluster em compara√ß√£o com outros clusters. Varia de -1 a 1. Valores mais altos s√£o melhores."
        )
    with col2:
        st.metric(
            label="Davies-Bouldin Index",
            value=f"{davies_bouldin_score(filtered_data, filtered_labels):.3f}",
            help="Mede a semelhan√ßa m√©dia entre clusters. Valores mais baixos s√£o melhores, indicando que os clusters est√£o bem separados."
        )

    st.markdown("---")
    st.markdown("### üé® Visualiza√ß√£o dos Clusters")
    st.markdown("Aqui podemos ver os clusters plotados nos dois primeiros componentes principais. Cada cor representa um cluster diferente.")
    
    # Adicionar os labels ao dataframe de PCA para visualiza√ß√£o
    df_plot = data.copy()
    df_plot['cluster'] = labels

    fig, ax = plt.subplots(figsize=(12, 8))
    sns.scatterplot(
        data=df_plot,
        x='PC_1',
        y='PC_2',
        hue='cluster',
        palette=sns.color_palette("hsv", n_colors=len(set(labels))),
        legend='full',
        alpha=0.7,
        ax=ax
    )
    ax.set_title("Clusters Visualizados em 2D")
    st.pyplot(fig)

    st.markdown("### üé∂ Explorar Clusters")
    st.markdown("Selecione um cluster para ver algumas das m√∫sicas que pertencem a ele.")

    cluster_id = st.selectbox("Selecione um Cluster ID:", sorted(set(labels)))
    
    # Adicionar os labels ao dataframe original para an√°lise
    df_original_with_clusters = df.iloc[data.index].copy()
    df_original_with_clusters['cluster'] = labels
    
    musicas_no_cluster = df_original_with_clusters[df_original_with_clusters['cluster'] == cluster_id]
    st.write(f"Mostrando as primeiras 15 m√∫sicas do Cluster {cluster_id}:")
    st.dataframe(musicas_no_cluster[['track_name', 'artists', 'track_genre', 'popularity']])


paginas = {
    "1. Vis√£o Geral dos Dados": pagina_1_visao_geral,
    "2. An√°lise Univariada": pagina_2_analise_univariada,
    "3. Correla√ß√£o entre Vari√°veis": pagina_3_correlacao,
    "4. Detec√ß√£o de Outliers": pagina_4_outliers,
    "5. Pr√©-processamento": pagina_5_preprocessamento,
    "6. Redu√ß√£o de Dimensionalidade": pagina_6_reducao_dimensionalidade,
    "7. Clusteriza√ß√£o": pagina_7_clusterizacao,
    "8. Avalia√ß√£o dos Clusters": pagina_8_avaliacao_clusters
}

st.sidebar.title("üìä EDA TuneTAP")
escolha = st.sidebar.radio("Escolha uma etapa da an√°lise:", list(paginas.keys()))

# Inicializar st.session_state se n√£o existir
if 'processed_df' not in st.session_state:
    st.session_state['processed_df'] = None
if 'pca_df' not in st.session_state:
    st.session_state['pca_df'] = None
if 'cluster_labels' not in st.session_state:
    st.session_state['cluster_labels'] = None
if 'cluster_data' not in st.session_state:
    st.session_state['cluster_data'] = None

paginas[escolha](df)
