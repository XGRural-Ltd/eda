import streamlit as st
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import kagglehub
import shap

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score

cols_dict = {'track_id' : 'Track ID',
             'artists' : 'Artists',
             'album_name' : 'Album Name',
             'track_name' : 'Track Name',
             'popularity' : 'Popularity',
             'duration_ms' : 'Duration (ms)',
             'explicit' : 'Explicit',
             'danceability' : 'Danceability',
             'energy' : 'Energy',
             'key' : 'Key',
             'loudness' : 'Loudness',
             'mode' : 'Mode',
             'speechiness' : 'Speechiness',
             'acousticness' : 'Acousticness',
             'instrumentalness' : 'Instrumentalness',
             'liveness' : 'Liveness',
             'valence' : 'Valence',
             'tempo' : 'Tempo',
             'time_signature' : 'Time Signature',
             'track_genre' : 'Track Genre'}

@st.cache_data
def load_data():
    path = kagglehub.dataset_download("maharshipandya/-spotify-tracks-dataset")
    path_dataset = path + '/dataset.csv'
    df = pd.read_csv(path_dataset, index_col=0)
    if 'track_genre' not in df.columns:
        df['track_genre'] = 'unknown'
    return df

df = load_data()

num_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
num_features = [col for col in num_features if col not in ['explicit', 'mode', 'key']]

def pagina_1_visao_geral(df):
    st.subheader("üìä Informa√ß√µes Gerais do Dataset")
    st.write("Nesta etapa vamos ficar mais familiarizados com os dados. Vamos explorar as colunas, tipos de dados, valores ausentes, estat√≠sticas descritivas e visualizar alguns plots.")

    num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
    

    st.markdown("**Visualize o DataFrame com as colunas selecionadas:**")
    cols = st.multiselect("Colunas para exibir:", df.columns.tolist(), default=df.columns[:6].tolist())
    st.dataframe(df[cols].head(15), hide_index=True)

    st.markdown("**Estat√≠sticas descritivas:**")
    desc = df.describe().rename(columns=cols_dict).T
    desc = desc.drop(columns=['count'])
    desc = desc.map(lambda x: f"{x:.2f}" if isinstance(x, (int, float, np.floating)) else x)
    st.table(desc)

    st.markdown("---")
    st.subheader("üßæ Dicion√°rio de Dados: Descri√ß√£o das Colunas")

    col_descriptions = {
        "track_id": "O ID do Spotify para a faixa.",
        "artists": "Nomes dos artistas que performaram a faixa. Se houver mais de um, s√£o separados por ponto e v√≠rgula.",
        "album_name": "Nome do √°lbum no qual a faixa aparece.",
        "track_name": "Nome da faixa.",
        "popularity": "Popularidade da faixa (0 a 100), baseada em n√∫mero e rec√™ncia de reprodu√ß√µes.",
        "duration_ms": "Dura√ß√£o da faixa em milissegundos.",
        "explicit": "Indica se a faixa possui conte√∫do expl√≠cito (True = sim, False = n√£o).",
        "danceability": "Qu√£o dan√ßante √© a faixa, de 0.0 (menos) a 1.0 (mais dan√ßante).",
        "energy": "Energia percebida da faixa, de 0.0 a 1.0.",
        "key": "Tom da m√∫sica (0 = D√≥, 1 = D√≥‚ôØ/R√©‚ô≠, ..., -1 = indetect√°vel).",
        "loudness": "Volume geral da faixa em decib√©is (dB).",
        "mode": "Modalidade: 1 = maior, 0 = menor.",
        "speechiness": "Detecta presen√ßa de fala. 1.0 = fala pura; 0.0 = m√∫sica pura.",
        "acousticness": "Confian√ßa de que a faixa √© ac√∫stica (0.0 a 1.0).",
        "instrumentalness": "Probabilidade de n√£o conter vocais. Pr√≥ximo de 1.0 = instrumental.",
        "liveness": "Probabilidade de ter sido gravada ao vivo. Acima de 0.8 = performance ao vivo.",
        "valence": "Qu√£o positiva √© a m√∫sica (0.0 = triste, 1.0 = alegre).",
        "tempo": "Tempo estimado da faixa (batidas por minuto).",
        "time_signature": "Compasso estimado (de 3 a 7).",
        "track_genre": "G√™nero musical da faixa."
    }

    available_columns = list(col_descriptions.keys())
    selected_columns = st.multiselect("Selecione as colunas que deseja entender melhor:", available_columns)

    if selected_columns:
        for col in selected_columns:
            st.markdown(f"**üîπ {col}**: {col_descriptions[col]}")
    
    st.markdown("---")
    st.subheader("üìà Visualiza√ß√µes Gerais de Distribui√ß√£o")

    selected_col = st.selectbox("Selecione uma coluna num√©rica:", num_cols)
    selected_col_name = cols_dict.get(selected_col)
    plot_type = st.radio("Tipo de gr√°fico:", ["Histograma", "Boxplot", "Ambos"])
    if plot_type == "Ambos":
        # Show histogram
        fig_hist, ax_hist = plt.subplots(figsize=(7, 4))
        sns.histplot(df[selected_col], kde=True, ax=ax_hist, color='skyblue')
        ax_hist.set_xlabel(selected_col_name)
        ax_hist.set_ylabel("Frequ√™ncia")
        ax_hist.set_title(f"Histograma de {selected_col_name}")
        st.pyplot(fig_hist)

        # Show boxplot
        fig_box, ax_box = plt.subplots(figsize=(7, 4))
        sns.boxplot(x=df[selected_col], ax=ax_box, color='salmon')
        ax_box.set_xlabel(selected_col_name)
        ax_box.set_ylabel("Valor")
        ax_box.set_title(f"Boxplot de {selected_col_name}")
        st.pyplot(fig_box)
    else:
        fig, ax = plt.subplots(figsize=(7, 4))
        if plot_type == "Histograma":
            sns.histplot(df[selected_col], kde=True, ax=ax, color='skyblue')
            ax.set_xlabel(selected_col_name)
            ax.set_ylabel("Frequ√™ncia")
            ax.set_title(f"Histograma de {selected_col_name}")
        else:
            sns.boxplot(x=df[selected_col], ax=ax, color='salmon')
            ax.set_xlabel(selected_col_name)
            ax.set_ylabel("Valor")
            ax.set_title(f"Boxplot de {selected_col_name}")
        st.pyplot(fig)

    st.markdown("---")
    st.subheader("üìà Gr√°ficos de Dispers√£o entre Vari√°veis Num√©ricas")
    x_axis = st.selectbox("Vari√°vel no eixo X:", num_features, index=0)
    y_axis = st.selectbox("Vari√°vel no eixo Y:", num_features, index=1)
    x_axis_name = cols_dict.get(x_axis)
    y_axis_name = cols_dict.get(y_axis)
    show_trend = st.checkbox("Mostrar linha de tend√™ncia (regress√£o linear)")
    fig, ax = plt.subplots()
    if x_axis == y_axis:
        # Plot a scatter of the variable against itself (diagonal line)
        sns.scatterplot(data=df, x=x_axis, y=y_axis, alpha=0.5, color='red', ax=ax)
        ax.plot(df[x_axis], df[y_axis], color='blue', linewidth=1, alpha=0.5)
    else:
        if show_trend:
            sns.regplot(data=df, x=x_axis, y=y_axis, ax=ax,
                        scatter_kws={'alpha': 0.5, 'color': 'red'},
                        line_kws={"color": "blue"})
        else:
            sns.scatterplot(data=df, x=x_axis, y=y_axis, alpha=0.5, color='red', ax=ax)
    ax.set_xlabel(x_axis_name)
    ax.set_ylabel(y_axis_name)
    ax.set_title(f"Dispers√£o entre {x_axis_name} e {y_axis_name}")
    st.pyplot(fig)

    st.markdown("üìå Danceability vs. Energy")
    st.markdown("- J√° esperamnos uma correla√ß√£o positiva entre 'danceability' e 'energy', pois m√∫sicas mais dan√ßantes tendem a ter mais energia.  \n"
                "- A linha de tend√™ncia (regress√£o linear) ajuda a visualizar uma correla√ß√£o moderadamente positiva. \n"
                "\n" 
                "üìå Acousticness vs. Energy \n"
                "- Correla√ß√£o negativa forte esperada, pois m√∫sicas ac√∫sticas s√£o menos energ√©ticas \n"
                "- A linha de tend√™ncia decrescemte indica uma rela√ß√£o inversamente proporcional. \n"
                "\n"
                "üìå Loudness vs. Energy \n"
                "- Baixa dispers√£o e ascend√™ncia dos pontos mostram uma correla√ß√£o fortemente positiva (m√∫sicas energ√©ticas costumam ser mais altas) \n")


def pagina_2_analise_univariada(df):
    st.subheader("üî¨ An√°lise Univariada Detalhada")
    st.markdown("Explore a distribui√ß√£o de cada vari√°vel. Use os filtros para comparar diferentes g√™neros e ajuste os gr√°ficos para uma an√°lise mais profunda.")

    st.markdown("### üé≠ Comparar Distribui√ß√µes por G√™nero")
    genres = sorted(df['track_genre'].unique().tolist())
    genres_cap = [g.capitalize() for g in genres]
    genre_map = dict(zip(genres_cap, genres))
    genre_filter = st.text_input("Filtrar g√™neros (digite parte do nome):", "")
    filtered_genres = [g for g in genres_cap if genre_filter.lower() in g.lower()]
    if not filtered_genres:
        st.warning("Nenhum g√™nero encontrado com esse filtro.")
        filtered_genres = genres_cap  # Mostra todos se filtro vazio

    genres_to_compare_cap = st.multiselect(
        "Selecione um ou mais g√™neros para comparar (opcional):",
        filtered_genres
    )
    genres_to_compare = [genre_map[g] for g in genres_to_compare_cap]

    if genres_to_compare:
        df_filtered = df[df['track_genre'].isin(genres_to_compare)]
        hue_on = 'track_genre'
    else:
        df_filtered = df.copy()
        hue_on = None

    st.markdown("### ‚öôÔ∏è Controles da An√°lise")
    selected_var = st.selectbox("Selecione uma vari√°vel num√©rica para an√°lise:", num_features)
    col_descriptions = {
        "popularity": "Popularidade da faixa (0 a 100), baseada em n√∫mero e rec√™ncia de reprodu√ß√µes.",
        "duration_ms": "Dura√ß√£o da faixa em milissegundos.",
        "danceability": "Qu√£o dan√ßante √© a faixa, de 0.0 (menos dan√ßante) a 1.0 (mais dan√ßante).",
        "energy": "Energia percebida da faixa, de 0.0 a 1.0.",
        "key": "Tom da m√∫sica (0 = D√≥, 1 = D√≥‚ôØ/R√©‚ô≠, ..., -1 = indetect√°vel).",
        "loudness": "Volume geral da faixa em decib√©is (dB).",
        "mode": "Modalidade: 1 = maior, 0 = menor.",
        "speechiness": "Detecta presen√ßa de fala. 1.0 = fala pura; 0.0 = m√∫sica pura.",
        "acousticness": "Confian√ßa de que a faixa √© ac√∫stica (0.0 a 1.0).",
        "instrumentalness": "Probabilidade de n√£o conter vocais. Pr√≥ximo de 1.0 = instrumental.",
        "liveness": "Probabilidade de ter sido gravada ao vivo. Acima de 0.8 = performance ao vivo.",
        "valence": "Qu√£o positiva √© a m√∫sica (0.0 = triste, 1.0 = alegre).",
        "tempo": "Tempo estimado da faixa (batidas por minuto).",
        "time_signature": "Compasso estimado (de 3 a 7)."
    }
    if selected_var in col_descriptions:
        st.info(f"**Descri√ß√£o:** {col_descriptions[selected_var]}")
    col_descriptions = {
        "popularity": "Popularidade da faixa (0 a 100), baseada em n√∫mero e rec√™ncia de reprodu√ß√µes.",
        "duration_ms": "Dura√ß√£o da faixa em milissegundos.",
        "danceability": "Qu√£o dan√ßante √© a faixa, de 0.0 (menos) a 1.0 (mais dan√ßante).",
        "energy": "Energia percebida da faixa, de 0.0 a 1.0.",
        "key": "Tom da m√∫sica (0 = D√≥, 1 = D√≥‚ôØ/R√©‚ô≠, ..., -1 = indetect√°vel).",
        "loudness": "Volume geral da faixa em decib√©is (dB).",
        "mode": "Modalidade: 1 = maior, 0 = menor.",
        "speechiness": "Detecta presen√ßa de fala. 1.0 = fala pura; 0.0 = m√∫sica pura.",
        "acousticness": "Confian√ßa de que a faixa √© ac√∫stica (0.0 a 1.0).",
        "instrumentalness": "Probabilidade de n√£o conter vocais. Pr√≥ximo de 1.0 = instrumental.",
        "liveness": "Probabilidade de ter sido gravada ao vivo. Acima de 0.8 = performance ao vivo.",
        "valence": "Qu√£o positiva √© a m√∫sica (0.0 = triste, 1.0 = alegre).",
        "tempo": "Tempo estimado da faixa (batidas por minuto).",
        "time_signature": "Compasso estimado (de 3 a 7)."
    }
    if selected_var in col_descriptions:
        st.info(f"**Descri√ß√£o:** {col_descriptions[selected_var]}")

    num_bins = st.slider("N√∫mero de Bins para o Histograma:", min_value=10, max_value=100, value=30)
    
    st.markdown(f"---\n### üéØ An√°lise da vari√°vel: `{selected_var}`")

    st.markdown("**üìä Visualiza√ß√£o da Distribui√ß√£o:**")
    col1, col2 = st.columns(2)
    with col1:
        fig1, ax1 = plt.subplots()
        if hue_on:
            sns.histplot(data=df_filtered, x=selected_var, hue=hue_on, kde=True, ax=ax1, bins=num_bins, palette='viridis')
        else:
            sns.histplot(data=df_filtered, x=selected_var, kde=True, ax=ax1, bins=num_bins, color='skyblue')
        ax1.set_title(f"Distribui√ß√£o de {selected_var}")
        st.pyplot(fig1)

    with col2:
        fig2, ax2 = plt.subplots(figsize=(7, 4))
        if hue_on:
            sns.boxplot(data=df_filtered, x=selected_var, y=hue_on, ax=ax2, palette='viridis', orient='h')
        else:
            sns.boxplot(data=df_filtered, x=selected_var, ax=ax2, color='skyblue', orient='h')
        ax2.set_title(f"Boxplot de {selected_var}")
        st.pyplot(fig2)

    st.markdown(f"**üîé Estat√≠sticas e Outliers para `{selected_var}`**")
    if genres_to_compare:
        st.write("Estat√≠sticas descritivas por g√™nero selecionado:")
        st.write(df_filtered.groupby('track_genre')[selected_var].describe().T)
    else:
        st.write("Estat√≠sticas descritivas gerais:")
        st.write(df_filtered[selected_var].describe())
    
    if st.checkbox(f"Mostrar outliers (m√∫sicas com valores extremos) para '{selected_var}'"):
        q1 = df_filtered[selected_var].quantile(0.25)
        q3 = df_filtered[selected_var].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        outliers = df_filtered[(df_filtered[selected_var] < lower_bound) | (df_filtered[selected_var] > upper_bound)]
        
        if outliers.empty:
            st.success("N√£o foram encontrados outliers com base no crit√©rio de 1.5 * IQR. ‚ú®")
        else:
            st.write(f"Foram encontrados **{len(outliers)}** outliers:")
            cols_to_show = ['track_name', 'artists', 'track_genre', selected_var]
            if st.checkbox("Mostrar todas as colunas dos outliers"):
                st.dataframe(outliers)
            else:
                st.dataframe(outliers[cols_to_show].sort_values(by=selected_var, ascending=False))

    if st.checkbox("Mostrar dicion√°rio das vari√°veis num√©ricas"):
        st.table(pd.DataFrame.from_dict(col_descriptions, orient='index', columns=['Descri√ß√£o']))

def pagina_3_correlacao(df):
    st.subheader("‚ÜîÔ∏è An√°lise de Correla√ß√£o")
    st.markdown("Investigue a rela√ß√£o entre as vari√°veis. Use o filtro de g√™nero e o slider de intensidade para focar nas correla√ß√µes mais importantes.")
    
    st.markdown("### üéµ Filtrar por G√™nero")
    genre_list = ['Todos'] + sorted(df['track_genre'].unique().tolist())
    corr_genre = st.selectbox("Selecione um g√™nero para calcular a correla√ß√£o:", genre_list)

    if corr_genre == 'Todos':
        df_corr = df[num_features]
    else:
        df_corr = df[df['track_genre'] == corr_genre][num_features]
        st.info(f"Mostrando correla√ß√µes apenas para o g√™nero: **{corr_genre}**")

    st.markdown("### ‚öôÔ∏è Controles do Heatmap")
    corr_method = st.selectbox("M√©todo de correla√ß√£o:", ["pearson", "spearman", "kendall"])
    
    corr_threshold = st.slider("Ocultar no gr√°fico e na tabela correla√ß√µes com valor absoluto abaixo de:", 0.0, 1.0, 0.0, 0.05)
    
    if not df_corr.empty and len(df_corr) > 1:
        corr_matrix = df_corr.corr(method=corr_method)
        
        mask_upper = np.triu(np.ones_like(corr_matrix, dtype=bool))
        mask_threshold = np.abs(corr_matrix) < corr_threshold
        mask_heatmap = mask_upper | mask_threshold
        
        fig, ax = plt.subplots(figsize=(14, 10))
        sns.heatmap(corr_matrix, mask=mask_heatmap, annot=True, cmap='coolwarm', fmt=".2f", ax=ax, annot_kws={"size": 10}, vmin=-1, vmax=1, linewidths=0.5, linecolor='gray')
        ax.set_title(f"Mapa de Calor (M√©todo: {corr_method.capitalize()}, G√™nero: {corr_genre})", fontsize=16)
        st.pyplot(fig)

        st.markdown("### ‚ú® Pares com Maior Correla√ß√£o")
        st.write(f"Abaixo est√£o os pares de vari√°veis com correla√ß√£o absoluta acima de **{corr_threshold}** (excluindo duplicatas e auto-correla√ß√µes).")
        
        mask_table = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
        corr_unstacked = corr_matrix.where(mask_table).stack()
        
        strong_pairs = corr_unstacked.sort_values(key=abs, ascending=False);
        
        strong_pairs = strong_pairs[abs(strong_pairs) > corr_threshold]

        if strong_pairs.empty:
            st.warning("Nenhum par encontrado acima do threshold. Tente um valor menor.")
        else:
            df_corr_pairs = strong_pairs.reset_index()
            df_corr_pairs.columns = ['Vari√°vel 1', 'Vari√°vel 2', 'Valor da Correla√ß√£o']
            st.dataframe(df_corr_pairs.head(20))

    else:
        st.warning(f"N√£o h√° dados suficientes para o g√™nero '{corr_genre}' para calcular a correla√ß√£o.")

def pagina_4_outliers(df):
    st.subheader("üö® Detec√ß√£o de Outliers com Isolation Forest")
    st.markdown("""
    Nesta se√ß√£o, vamos identificar **outliers**: m√∫sicas que possuem caracter√≠sticas muito diferentes da maioria. Um outlier pode ser uma m√∫sica experimental, um erro nos dados ou simplesmente uma faixa √∫nica.
    
    Usaremos o algoritmo **Isolation Forest**, que √© eficiente em detectar anomalias em dados multidimensionais. Ele funciona isolando observa√ß√µes ao selecionar aleatoriamente uma feature e, em seguida, um valor de divis√£o aleat√≥rio entre os valores m√°ximo e m√≠nimo da feature selecionada.
    """)

    st.markdown("### ‚öôÔ∏è Controles da Detec√ß√£o")
    features_for_outliers = st.multiselect(
        "Selecione as features para a detec√ß√£o de outliers:", 
        num_features, 
        default=['danceability', 'energy', 'loudness', 'acousticness', 'valence']
    )
    
    if not features_for_outliers:
        st.warning("Por favor, selecione ao menos uma feature para a an√°lise.")
        return

    contamination = st.slider(
        "Taxa de contamina√ß√£o (propor√ß√£o de outliers):", 
        0.01, 0.2, 0.05, step=0.01,
        help="Este valor representa a propor√ß√£o esperada de outliers no dataset. Um valor maior resultar√° em mais m√∫sicas sendo classificadas como anomalias."
    )

    st.markdown("---")
    st.markdown("### üéº M√∫sicas An√¥malas Detectadas")

    df_outlier_analysis = df.copy()

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_outlier_analysis[features_for_outliers])

    iso = IsolationForest(contamination=contamination, random_state=42)
    df_outlier_analysis['anomaly'] = iso.fit_predict(X_scaled)
    outliers = df_outlier_analysis[df_outlier_analysis['anomaly'] == -1]

    st.write(f"Com base nas suas configura√ß√µes, foram detectadas **{len(outliers)}** m√∫sicas como outliers.")
    st.dataframe(outliers[['track_name', 'artists', 'track_genre'] + features_for_outliers])

    if len(features_for_outliers) >= 2:
        st.subheader("üìç Visualiza√ß√£o de Outliers em 2D (usando PCA)")
        st.markdown("""
        Para visualizar os outliers em um gr√°fico 2D, reduzimos a dimensionalidade das features selecionadas usando **An√°lise de Componentes Principais (PCA)**. Os pontos em vermelho representam as m√∫sicas marcadas como outliers.
        """)
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_scaled)
        
        df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=df_outlier_analysis.index)
        df_pca['anomaly'] = df_outlier_analysis['anomaly']
        
        fig, ax = plt.subplots(figsize=(10, 7))
        sns.scatterplot(
            data=df_pca, x='PC1', y='PC2', hue='anomaly', 
            palette={1: 'blue', -1: 'red'}, style='anomaly',
            markers={1: '.', -1: 'X'}, s=100, alpha=0.7, ax=ax
        )
        ax.set_title("Outliers detectados via PCA")
        ax.legend(['Normal', 'Outlier'])
        st.pyplot(fig)
    else:
        st.info("Selecione 2 ou mais features para visualizar o gr√°fico de dispers√£o com PCA.")

    st.info("Nem todo outlier deve ser removido: em m√∫sica, valores extremos podem indicar faixas inovadoras ou de nicho, enriquecendo a an√°lise. Remover outliers pode eliminar informa√ß√µes valiosas sobre diversidade musical.")

@st.cache_data
def convert_df_to_csv(df):
    return df.to_csv(index=False).encode('utf-8')

def pagina_5_preprocessamento(df):
    st.subheader("‚öôÔ∏è 5. Pr√©-processamento dos Dados")
    st.markdown("""
    O pr√©-processamento √© uma etapa fundamental na prepara√ß√£o de dados para modelos de Machine Learning. Aqui, transformaremos nossas features para que os algoritmos possam interpret√°-las da melhor forma poss√≠vel.
    
    Vamos abordar tr√™s etapas principais:
    1.  **Imputa√ß√£o de Dados**: Substituir valores ausentes.
    2.  **Feature Scaling**: Padronizar as escalas das nossas vari√°veis num√©ricas.
    3.  **One-Hot Encoding**: Converter vari√°veis categ√≥ricas em um formato num√©rico.
    """)

    df_processed = df.copy()

    st.markdown("---")
    st.markdown("### 1. Tratamento de Valores Ausentes")
    st.markdown("""
    Antes de escalar os dados, precisamos lidar com valores ausentes (NaN). Uma estrat√©gia comum e robusta √© a **imputa√ß√£o pela mediana**, onde substitu√≠mos os valores ausentes pelo valor central da coluna. Isso √© menos sens√≠vel a outliers do que usar a m√©dia.
    """)
    
    # Preencher NaNs com a mediana das colunas num√©ricas
    numeric_cols_with_na = df_processed[num_features].columns[df_processed[num_features].isnull().any()].tolist()
    if numeric_cols_with_na:
        st.write("Valores ausentes encontrados nas seguintes colunas e preenchidos com a mediana:")
        st.write(df_processed[numeric_cols_with_na].isnull().sum().to_frame(name='NAs Preenchidos'))
        df_processed.fillna(df_processed.median(numeric_only=True), inplace=True)
    else:
        st.success("Nenhum valor ausente encontrado nas colunas num√©ricas. ‚úÖ")
    

    st.markdown("---")
    st.markdown("### 2. Feature Scaling (Padroniza√ß√£o)")
    st.markdown("""
    Algoritmos de clusteriza√ß√£o, como o K-Means, s√£o sens√≠veis √† escala das features. Vari√°veis com escalas maiores (como `duration_ms`) podem dominar o processo de agrupamento.
    Usaremos o **StandardScaler**, que transforma os dados para que tenham m√©dia 0 e desvio padr√£o 1.
    """)

    features_to_scale = st.multiselect(
        "Selecione as features num√©ricas para padronizar:",
        options=num_features,
        default=num_features
    )
    
    if features_to_scale:
        scaler = StandardScaler()
        st.markdown("**StandardScaler** transforma os dados para m√©dia 0 e desvio padr√£o 1 (padroniza√ß√£o).")
    else:
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        st.markdown("**MinMaxScaler** transforma os dados para o intervalo [0, 1] (normaliza√ß√£o).")
    df_processed[features_to_scale] = scaler.fit_transform(df_processed[features_to_scale])

        st.markdown("**Compara√ß√£o: Antes vs. Depois da Padroniza√ß√£o** (para a feature `danceability`)")
        col1, col2 = st.columns(2)
        with col1:
            st.write("Antes:")
            fig, ax = plt.subplots(figsize=(6,4))
            sns.histplot(df['danceability'], kde=True, ax=ax, color='blue')
            ax.set_title("Original")
            st.pyplot(fig)
        with col2:
            st.write("Depois:")
            fig, ax = plt.subplots(figsize=(6,4))
            sns.histplot(df_processed['danceability'], kde=True, ax=ax, color='green')
            ax.set_title("Padronizado")
            st.pyplot(fig)
    
    st.markdown("---")
    st.markdown("### 3. One-Hot Encoding para G√™neros")
    st.markdown("""
    Para usar a feature `track_genre` em nosso modelo, precisamos convert√™-la de texto para um formato num√©rico. O **One-Hot Encoding** cria novas colunas para cada g√™nero, marcando com `1` se a m√∫sica pertence √†quele g√™nero e `0` caso contr√°rio.
    """)
    
    if st.checkbox("Aplicar One-Hot Encoding na coluna 'track_genre'?", value=True):
        df_processed = pd.get_dummies(df_processed, columns=['track_genre'], prefix='genre')
        st.success(f"One-Hot Encoding aplicado! Novas colunas de g√™nero foram criadas.")
    
    st.markdown("---")
    st.markdown("### üèÅ DataFrame Final Pr√©-processado")
    st.markdown("Abaixo est√° uma amostra do nosso dataset ap√≥s as transforma√ß√µes. Este √© o conjunto de dados que usaremos para a clusteriza√ß√£o.")
    
    final_features = df_processed.select_dtypes(include=np.number).columns.tolist()
    final_df = df_processed[final_features]

    st.dataframe(final_df.head())
    st.write(f"O dataset final possui **{final_df.shape[0]}** linhas e **{final_df.shape[1]}** features.")

    st.markdown("### üíæ Baixar Dados Processados")
    st.markdown("Clique no bot√£o para baixar o DataFrame processado em um arquivo CSV para uso posterior.")
    
    csv = convert_df_to_csv(final_df)
    st.download_button(
       label="Baixar dados como CSV",
       data=csv,
       file_name='processed_spotify_data.csv',
       mime='text/csv',
    )
    if st.button("Salvar DataFrame em cache para pr√≥ximas etapas"):
        st.session_state['processed_df'] = final_df
        st.success("DataFrame processado salvo na sess√£o! ‚úÖ")

def pagina_6_reducao_dimensionalidade(df):
    st.subheader("üìâ 6. Redu√ß√£o de Dimensionalidade")
    st.markdown("""
    Com um grande n√∫mero de features, pode ser dif√≠cil visualizar e modelar os dados. A **Redu√ß√£o de Dimensionalidade** nos ajuda a "comprimir" as informa√ß√µes mais importantes em um n√∫mero menor de componentes.
    
    Usaremos a **An√°lise de Componentes Principais (PCA)**, uma t√©cnica popular que encontra novas eixos (componentes) que maximizam a vari√¢ncia nos dados.
    """)
    
    if 'processed_df' not in st.session_state or st.session_state['processed_df'] is None:
        st.warning("Por favor, execute o pr√©-processamento na p√°gina '5. Pr√©-processamento' e clique em 'Salvar DataFrame' antes de continuar.")
        return

    processed_df = st.session_state['processed_df']
    
    st.markdown("### ‚öôÔ∏è Configurando o PCA")
    n_components = st.slider(
        "N√∫mero de componentes principais para gerar:",
        min_value=2, max_value=20, value=10,
        help="Escolha quantos componentes (novas features) voc√™ deseja criar. Come√ßar com 10 a 15 √© geralmente um bom ponto de partida."
    )
    
    reduction_method = st.selectbox("M√©todo de redu√ß√£o de dimensionalidade:", ["PCA", "t-SNE"])
    if reduction_method == "PCA":
        pca = PCA(n_components=n_components)
        X_pca = pca.fit_transform(processed_df)

        st.markdown("### üìä Vari√¢ncia Explicada")
        st.markdown("O gr√°fico acima mostra a vari√¢ncia explicada por cada componente. Use-o para decidir quantos componentes manter.")

        explained_variance = pca.explained_variance_ratio_
        cumulative_variance = np.cumsum(explained_variance)

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.bar(range(1, n_components + 1), explained_variance, alpha=0.6, color='b', label='Vari√¢ncia Individual')
        ax.plot(range(1, n_components + 1), cumulative_variance, 'r-o', label='Vari√¢ncia Cumulativa')
        ax.set_xlabel('Componentes Principais')
        ax.set_ylabel('Propor√ß√£o da Vari√¢ncia Explicada')
        ax.set_title('Vari√¢ncia Explicada pelos Componentes Principais')
        ax.legend(loc='best')
        ax.set_xticks(range(1, n_components + 1))
        st.pyplot(fig)

        st.info(f"Com **{n_components}** componentes, conseguimos explicar **{cumulative_variance[-1]:.2%}** da vari√¢ncia total dos dados.")
        
        st.markdown("### üíø Dados Transformados pelo PCA")
        st.markdown("Abaixo est√° o nosso dataset transformado, agora com um n√∫mero reduzido de dimens√µes. Estes ser√£o os dados que usaremos para a clusteriza√ß√£o.")
        
        df_pca = pd.DataFrame(X_pca, columns=[f'PC_{i+1}' for i in range(n_components)])
        st.dataframe(df_pca.head())

    if st.button("Salvar dados do PCA para pr√≥ximas etapas"):
        st.session_state['pca_df'] = df_pca
        st.success("Dados transformados pelo PCA salvos na sess√£o! ‚úÖ")


def pagina_7_clusterizacao(df):
    st.subheader("üß© 7. Clusteriza√ß√£o")
    st.markdown("""
    Agora que nossos dados est√£o preparados, vamos aplicar algoritmos de **clusteriza√ß√£o** para encontrar grupos (clusters) de m√∫sicas com caracter√≠sticas semelhantes. O objetivo √© descobrir "playlists" naturais escondidas nos dados.
    """)

    if 'pca_df' not in st.session_state or st.session_state['pca_df'] is None:
        st.warning("Por favor, execute a Redu√ß√£o de Dimensionalidade na p√°gina '6. Redu√ß√£o de Dimensionalidade' e salve os dados antes de continuar.")
        return
    
    X_data = st.session_state['pca_df']

    st.markdown("### ü§ñ Escolha do Algoritmo de Clusteriza√ß√£o")
    algo_choice = st.selectbox(
        "Selecione o algoritmo:",
        ["K-Means", "DBSCAN", "Clustering Aglomerativo"]
    )

    model = None
    labels = None

    if algo_choice == "K-Means":
        st.markdown("""
        **K-Means** √© um dos algoritmos mais populares. Ele agrupa os dados tentando separar as amostras em *k* grupos de vari√¢ncia igual, minimizando um crit√©rio conhecido como in√©rcia. Voc√™ precisa definir o n√∫mero de clusters (k) antecipadamente.
        """)
        k = st.slider("N√∫mero de clusters (k):", min_value=2, max_value=20, value=8)
        model = KMeans(n_clusters=k, random_state=42, n_init=10)
        
        if st.checkbox("Mostrar gr√°fico de cotovelo para escolha de k"):
            inertias = []
            ks = range(2, 15)
            for k_val in ks:
                km = KMeans(n_clusters=k_val, random_state=42, n_init=10)
                km.fit(X_data)
                inertias.append(km.inertia_)
            fig, ax = plt.subplots()
            ax.plot(ks, inertias, '-o')
            ax.set_xlabel("N√∫mero de clusters (k)")
            ax.set_ylabel("In√©rcia")
            ax.set_title("Gr√°fico de Cotovelo")
            st.pyplot(fig)
    elif algo_choice == "DBSCAN":
        st.markdown("""
        **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) agrupa pontos que est√£o densamente compactados, marcando como outliers os pontos que est√£o sozinhos em regi√µes de baixa densidade. √â √≥timo para encontrar clusters de formas arbitr√°rias e n√£o exige que voc√™ defina o n√∫mero de clusters.
        """)
        eps = st.slider("Epsilon (eps - raio da vizinhan√ßa):", min_value=0.1, max_value=5.0, value=1.5, step=0.1)
        min_samples = st.slider("N√∫mero M√≠nimo de Amostras (min_samples):", min_value=1, max_value=50, value=10)
        model = DBSCAN(eps=eps, min_samples=min_samples)
    elif algo_choice == "Clustering Aglomerativo":
        st.markdown("""
        O **Clustering Aglomerativo** realiza uma clusteriza√ß√£o hier√°rquica. Ele come√ßa tratando cada ponto como um cluster separado e, em seguida, mescla recursivamente os pares de clusters mais pr√≥ximos at√© que um certo n√∫mero de clusters seja alcan√ßado.
        """)
        n_clusters_agg = st.slider("N√∫mero de clusters:", min_value=2, max_value=20, value=8)
        model = AgglomerativeClustering(n_clusters=n_clusters_agg)

    if st.button(f"Executar {algo_choice}"):
        with st.spinner("Clusterizando os dados... Isso pode levar um momento."):
            labels = model.fit_predict(X_data)
            st.session_state['cluster_labels'] = labels
            st.session_state['cluster_data'] = X_data
            st.success(f"Clusteriza√ß√£o com {algo_choice} conclu√≠da! Os resultados foram salvos.")
            
            n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)
            st.write(f"N√∫mero de clusters encontrados: **{n_clusters_found}**")
            if -1 in labels:
                noise_points = np.sum(labels == -1)
                st.write(f"N√∫mero de pontos de ru√≠do (outliers): **{noise_points}**")

def pagina_8_avaliacao_clusters(df):
    st.subheader("üèÜ 8. Avalia√ß√£o dos Clusters")
    st.markdown("""
    Como saber se os clusters que encontramos s√£o bons? Nesta etapa, vamos usar m√©tricas quantitativas e visualiza√ß√µes para avaliar a qualidade dos nossos agrupamentos.
    """)

    if (
        'cluster_labels' not in st.session_state
        or st.session_state['cluster_labels'] is None
        or len(st.session_state['cluster_labels']) == 0
    ):
        st.warning("Por favor, execute a clusteriza√ß√£o na p√°gina '7. Clusteriza√ß√£o' antes de continuar.")
    if 'cluster_labels' not in st.session_state or st.session_state['cluster_labels'] is None or len(st.session_state['cluster_labels']) == 0:
        st.warning("Por favor, execute a clusteriza√ß√£o na p√°gina '7. Clusteriza√ß√£o' antes de continuar.")
        return

    labels = st.session_state['cluster_labels']
    X_data = st.session_state['cluster_data']

    st.markdown("### üìä Avalia√ß√£o Quantitativa")
    st.markdown("""
    Vamos usar duas m√©tricas populares para avaliar a qualidade dos clusters:
    - **Silhouette Score**: Mede qu√£o semelhantes s√£o os objetos dentro de um mesmo cluster em compara√ß√£o com objetos de outros clusters. Varia de -1 a 1.
    - **Davies-Bouldin Score**: Mede a compacta√ß√£o e separa√ß√£o dos clusters. Valores mais baixos indicam melhores agrupamentos.
    """)

    if st.checkbox("Calcular m√©tricas de avalia√ß√£o"):
        if len(set(labels)) > 1:
            silhouette_avg = silhouette_score(X_data, labels)
            davies_bouldin = davies_bouldin_score(X_data, labels)
            st.success(f"Silhouette Score: {silhouette_avg:.3f}")
            st.success(f"Davies-Bouldin Score: {davies_bouldin:.3f}")
        else:
            st.warning("N√£o √© poss√≠vel calcular as m√©tricas. Tente com mais clusters.")

    st.markdown("### üìâ Visualiza√ß√£o dos Clusters")
    st.markdown("""
    Uma imagem vale mais que mil palavras. Vamos visualizar os clusters em um gr√°fico 2D. Para isso, usaremos as duas primeiras componentes principais obtidas na redu√ß√£o de dimensionalidade.
    """)
    
    if 'pca_df' in st.session_state:
        df_pca = st.session_state['pca_df']
        
        fig, ax = plt.subplots(figsize=(10, 7))
        sns.scatterplot(
            data=df_pca, x='PC1', y='PC2', hue=labels, 
            palette='viridis', style=labels,
            markers={0: 'o', 1: 'X', 2: 's', 3: 'D', 4: '^', 5: 'v', 6: '<', 7: '>', -1: 'P'}, 
            s=100, alpha=0.7, ax=ax
        )
        ax.set_title("Visualiza√ß√£o dos Clusters Encontrados")
        ax.legend(title="Clusters")
        st.pyplot(fig)
    else:
        st.warning("Dados do PCA n√£o encontrados. Verifique a etapa de redu√ß√£o de dimensionalidade.")


PAGES = {
    "1. Vis√£o Geral": pagina_1_visao_geral,
    "2. An√°lise Univariada": pagina_2_analise_univariada,
    "3. Correla√ß√£o": pagina_3_correlacao,
    "4. Outliers": pagina_4_outliers,
    "5. Pr√©-processamento": pagina_5_preprocessamento,
    "6. Redu√ß√£o de Dimensionalidade": pagina_6_reducao_dimensionalidade,
    "7. Clusteriza√ß√£o": pagina_7_clusterizacao,
    "8. Avalia√ß√£o dos Clusters": pagina_8_avaliacao_clusters,
}

st.sidebar.title("Navega√ß√£o")
page = st.sidebar.radio("Escolha a p√°gina:", list(PAGES.keys()))
PAGES[page](df)
